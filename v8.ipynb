{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\ML\\Capstone\\Knowledge Distilization\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in fine tuning Device:  cuda\n",
      "[Teacher] Epoch 1, Train Loss: 0.42036667894124985\n",
      "[Teacher] Epoch 1, Val Loss: 0.29793220018125643, Accuracy: 0.8726, F1 Score: 0.8726\n",
      "[Teacher] Epoch 2, Train Loss: 0.24592397272586822\n",
      "[Teacher] Epoch 2, Val Loss: 0.2918499536444522, Accuracy: 0.8770, F1 Score: 0.8770\n",
      "[Teacher] Epoch 3, Train Loss: 0.18007266725301743\n",
      "[Teacher] Epoch 3, Val Loss: 0.3174219919060161, Accuracy: 0.8784, F1 Score: 0.8784\n",
      "in train Device:  cuda\n",
      "[KD] Epoch 1, Train Loss: 0.9149522017478943\n",
      "[KD] Epoch 1, Val Loss: 0.501279138455725, Accuracy: 0.7608, F1 Score: 0.7580\n",
      "[KD] Epoch 2, Train Loss: 0.561373873758316\n",
      "[KD] Epoch 2, Val Loss: 0.39204789185599914, Accuracy: 0.8202, F1 Score: 0.8193\n",
      "[KD] Epoch 3, Train Loss: 0.37892732998132705\n",
      "[KD] Epoch 3, Val Loss: 0.3776704253522074, Accuracy: 0.8380, F1 Score: 0.8379\n",
      "[KD] Epoch 4, Train Loss: 0.26362261098623274\n",
      "[KD] Epoch 4, Val Loss: 0.38059326009765554, Accuracy: 0.8448, F1 Score: 0.8448\n",
      "[KD] Epoch 5, Train Loss: 0.18550056738853454\n",
      "[KD] Epoch 5, Val Loss: 0.3743077752649025, Accuracy: 0.8504, F1 Score: 0.8503\n",
      "[KD] Epoch 6, Train Loss: 0.14069888627529145\n",
      "[KD] Epoch 6, Val Loss: 0.39854036478005395, Accuracy: 0.8464, F1 Score: 0.8464\n",
      "[KD] Epoch 7, Train Loss: 0.10966091089844704\n",
      "[KD] Epoch 7, Val Loss: 0.41069532864412683, Accuracy: 0.8434, F1 Score: 0.8432\n",
      "[KD] Epoch 8, Train Loss: 0.09555428229868412\n",
      "[KD] Epoch 8, Val Loss: 0.41118609077136986, Accuracy: 0.8438, F1 Score: 0.8438\n",
      "[KD] Epoch 9, Train Loss: 0.08475731675028801\n",
      "[KD] Epoch 9, Val Loss: 0.4163925836372907, Accuracy: 0.8464, F1 Score: 0.8464\n",
      "[KD] Epoch 10, Train Loss: 0.07813003663420677\n",
      "[KD] Epoch 10, Val Loss: 0.43189185573037264, Accuracy: 0.8426, F1 Score: 0.8426\n",
      "Early stopping triggered\n",
      "Student Model Efficiency Metrics:\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        aten::embedding        12.76%       8.791ms        47.43%      32.683ms      32.683ms       0.000us         0.00%      19.518us      19.518us             1  \n",
      "                                             aten::lstm        20.57%      14.172ms        34.65%      23.875ms      23.875ms       0.000us         0.00%       4.422ms       4.422ms             1  \n",
      "                                     aten::index_select        14.75%      10.164ms        34.63%      23.863ms      23.863ms       9.759us         0.21%      19.518us      19.518us             1  \n",
      "                                           Unrecognized        19.28%      13.287ms        19.28%      13.287ms      13.287ms       9.759us         0.21%       9.759us       9.759us             1  \n",
      "                        aten::_cudnn_rnn_flatten_weight         4.48%       3.084ms        16.49%      11.366ms      11.366ms       0.000us         0.00%      53.503us      53.503us             1  \n",
      "                                       aten::_cudnn_rnn         5.91%       4.073ms        13.98%       9.629ms       9.629ms       4.414ms        96.17%       4.422ms       4.422ms             1  \n",
      "                                            aten::zeros        11.19%       7.712ms        11.45%       7.886ms       2.629ms       0.000us         0.00%      10.912us       3.637us             3  \n",
      "                                       cudaLaunchKernel         4.22%       2.906ms         4.22%       2.906ms       5.484us       0.000us         0.00%       0.000us       0.000us           530  \n",
      "                                         cuLaunchKernel         3.10%       2.138ms         3.10%       2.138ms       4.144us       0.000us         0.00%       0.000us       0.000us           516  \n",
      "                                           aten::linear         0.06%      38.600us         0.62%     429.900us     214.950us       0.000us         0.00%      38.687us      19.344us             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 68.904ms\n",
      "Self CUDA time total: 4.590ms\n",
      "\n",
      "Student Model Parameter Count: 5363883\n",
      "[KD] Final Test Accuracy: 0.8313, F1 Score: 0.8311\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, DataCollatorWithPadding\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.profiler\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install gensim: pip install gensim\")\n",
    "\n",
    "# Required dependencies: pip install gensim torch transformers datasets numpy scikit-learn\n",
    "\n",
    "# Define RNN Student Model with Attention, Word2Vec Embeddings, Dropout, and Multi-Layer LSTM\n",
    "class RNNStudent(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, word2vec_model=None, tokenizer=None):\n",
    "        super(RNNStudent, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        if word2vec_model and tokenizer:\n",
    "            embedding_matrix = torch.randn(vocab_size, embed_dim) * 0.01\n",
    "            for token, idx in tokenizer.vocab.items():\n",
    "                if token in word2vec_model.wv:\n",
    "                    embedding_matrix[idx] = torch.from_numpy(word2vec_model.wv[token].copy())\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, _) = self.rnn(embedded)\n",
    "        attention_weights = torch.softmax(self.attention(output).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        context = torch.sum(output * attention_weights, dim=1)\n",
    "        return self.fc(context)\n",
    "\n",
    "# Hybrid Distillation Loss\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=3.0, alpha=0.4, beta=0.2):\n",
    "    soft_loss = nn.KLDivLoss(reduction='batchmean')(\n",
    "        nn.functional.log_softmax(student_logits / T, dim=1),\n",
    "        nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "    ) * (T * T)\n",
    "    hard_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "    sequence_loss = nn.CrossEntropyLoss()(student_logits, teacher_logits.argmax(dim=1))\n",
    "    return alpha * soft_loss + (1 - alpha - beta) * hard_loss + beta * sequence_loss\n",
    "\n",
    "# Fine-Tune Teacher Model with Multiple Learning Rates\n",
    "def fine_tune_teacher(teacher, train_loader, val_loader, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"in fine tuning Device: \", device)\n",
    "    teacher.train()\n",
    "    teacher.to(device)\n",
    "    param_groups = [\n",
    "        {\"params\": [p for n, p in teacher.named_parameters() if \"classifier\" not in n], \"lr\": 1e-5},\n",
    "        {\"params\": [p for n, p in teacher.named_parameters() if \"classifier\" in n], \"lr\": 5e-5}\n",
    "    ]\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        teacher.train()\n",
    "        for batch in train_loader:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = teacher(input_ids=texts, attention_mask=attention_mask).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}\")\n",
    "        teacher.eval()\n",
    "        preds, true_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = teacher(input_ids=texts, attention_mask=attention_mask).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(true_labels, preds)\n",
    "        f1 = f1_score(true_labels, preds, average='weighted')\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Val Loss: {val_loss/len(val_loader)}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Training Function for Knowledge Distillation with Early Stopping\n",
    "def train_kd(teacher, student, train_loader, val_loader, epochs=15, prune_amount=0.3, T=3.0, alpha=0.4, beta=0.2):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"in train Device: \", device)\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "    for name, module in student.named_modules():\n",
    "        if isinstance(module, nn.LSTM):\n",
    "            prune.l1_unstructured(module, name='weight_ih_l0', amount=prune_amount)\n",
    "            prune.l1_unstructured(module, name='weight_hh_l0', amount=prune_amount)\n",
    "            if hasattr(module, 'weight_ih_l1'):\n",
    "                prune.l1_unstructured(module, name='weight_ih_l1', amount=prune_amount)\n",
    "                prune.l1_unstructured(module, name='weight_hh_l1', amount=prune_amount)\n",
    "        elif isinstance(module, nn.Linear) and module != student.attention:\n",
    "            prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        student.train()\n",
    "        for batch in train_loader:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(input_ids=texts, attention_mask=attention_mask).logits\n",
    "            student_outputs = student(texts)\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, labels, T, alpha, beta)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[KD] Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}\")\n",
    "        student.eval()\n",
    "        preds, true_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = student(texts)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(true_labels, preds)\n",
    "        f1 = f1_score(true_labels, preds, average='weighted')\n",
    "        print(f\"[KD] Epoch {epoch+1}, Val Loss: {val_loss/len(val_loader)}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    student.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "                student(texts)\n",
    "            break\n",
    "    print(\"Student Model Efficiency Metrics:\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    param_count = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "    print(f\"Student Model Parameter Count: {param_count}\")\n",
    "\n",
    "# Load and Preprocess Data\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized = dataset.map(tokenize_function, batched=True)\n",
    "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
    "tokenized = tokenized.remove_columns([\"text\"])\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "# Split train into train and validation\n",
    "train_val_split = tokenized[\"train\"].train_test_split(test_size=0.2)\n",
    "train_dataset = train_val_split[\"train\"]\n",
    "val_dataset = train_val_split[\"test\"]\n",
    "test_dataset = tokenized[\"test\"]\n",
    "\n",
    "# Train Word2Vec on IMDB\n",
    "sentences = [text.split() for text in dataset['train']['text']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# DataLoader\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=data_collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=data_collator)\n",
    "\n",
    "# Initialize Models\n",
    "# Note: The classifier weights are randomly initialized and will be fine-tuned in fine_tune_teacher\n",
    "teacher = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "# Add dropout to classifier to reduce overfitting\n",
    "teacher.classifier = nn.Sequential(nn.Dropout(0.1), teacher.classifier)\n",
    "student = RNNStudent(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embed_dim=100,\n",
    "    hidden_dim=256,\n",
    "    output_dim=2,\n",
    "    word2vec_model=word2vec_model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-Tune Teacher and Perform Knowledge Distillation\n",
    "fine_tune_teacher(teacher, train_loader, val_loader, epochs=3)\n",
    "train_kd(teacher, student, train_loader, val_loader, epochs=15)\n",
    "\n",
    "# Final Test Evaluation\n",
    "student.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "preds, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        texts = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = student(texts)\n",
    "        preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "f1 = f1_score(true_labels, preds, average='weighted')\n",
    "print(f\"[KD] Final Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
