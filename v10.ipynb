{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[Teacher] Epoch 1: 100%|██████████| 782/782 [07:33<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 1, Train Loss: 0.3910652404684392\n",
      "[Teacher] Epoch 1, Val Loss: 0.28106234923405266, Accuracy: 0.8791, F1 Score: 0.8791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 2: 100%|██████████| 782/782 [07:31<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 2, Train Loss: 0.22204554941781493\n",
      "[Teacher] Epoch 2, Val Loss: 0.27620646771986773, Accuracy: 0.8881, F1 Score: 0.8881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 3: 100%|██████████| 782/782 [07:30<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 3, Train Loss: 0.11518096636332896\n",
      "[Teacher] Epoch 3, Val Loss: 0.3249402591069003, Accuracy: 0.8843, F1 Score: 0.8843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[KD] Epoch 1: 100%|██████████| 782/782 [05:36<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KD] Epoch 1, Train Loss: 1.130989343537699\n",
      "[KD] Epoch 1, Val Loss: 0.6689124113839605, Accuracy: 0.5914, F1 Score: 0.5914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[KD] Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cudnn RNN backward can only be called in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 262\u001b[39m\n\u001b[32m    259\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mParameter Count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_parameters(student)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 248\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    245\u001b[39m fine_tune_teacher(teacher, train_dataloader, val_dataloader, Config.device)\n\u001b[32m    247\u001b[39m \u001b[38;5;66;03m# Train student\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[43mtrain_student\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Load best student model\u001b[39;00m\n\u001b[32m    251\u001b[39m student.load_state_dict(torch.load(\u001b[33m\"\u001b[39m\u001b[33mbest_student.pt\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mtrain_student\u001b[39m\u001b[34m(teacher, student, train_dataloader, val_dataloader, device)\u001b[39m\n\u001b[32m    200\u001b[39m loss = compute_kd_loss(student_logits, teacher_logits, student_seq, teacher_seq, labels, attention_mask)\n\u001b[32m    201\u001b[39m total_loss += loss.item()\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m optimizer.step()\n\u001b[32m    204\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Capstone\\Knowledge Distilization\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Capstone\\Knowledge Distilization\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mg:\\ML\\Capstone\\Knowledge Distilization\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: cudnn RNN backward can only be called in training mode"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, DataCollatorWithPadding\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn.utils import prune\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 32\n",
    "    max_len = 128\n",
    "    teacher_epochs = 3\n",
    "    student_epochs = 10\n",
    "    teacher_lr = {\"bert\": 1e-5, \"classifier\": 5e-5}\n",
    "    student_lr = {\n",
    "        \"embedding\": 1e-4,  # Lower rate for pre-trained embeddings\n",
    "        \"rnn\": 5e-4,       # Higher rate for LSTM\n",
    "        \"attention\": 3e-4, # Moderate rate for attention\n",
    "        \"classifier\": 1e-3 # Higher rate for task-specific layer\n",
    "    }\n",
    "    warmup_steps = 0.1  # 10% of total steps\n",
    "    T = 3.2  # Temperature for KD\n",
    "    alpha = 0.4  # Weight for soft loss\n",
    "    beta = 0.2  # Weight for sequence loss\n",
    "    patience = 5  # Early stopping patience\n",
    "    embed_dim = 100  # Word2Vec embedding dimension\n",
    "    hidden_dim = 256  # RNN hidden dimension\n",
    "    dropout = 0.3  # Dropout rate\n",
    "    prune_amount = 0.3  # Pruning percentage\n",
    "\n",
    "# Load and preprocess IMDB dataset\n",
    "def load_imdb_data():\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def preprocess(example):\n",
    "        encoding = tokenizer(\n",
    "            example[\"text\"],\n",
    "            max_length=Config.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": example[\"label\"]\n",
    "        }\n",
    "    \n",
    "    train_dataset = dataset[\"train\"].map(preprocess)\n",
    "    val_dataset = dataset[\"test\"].map(preprocess)  # Using test split as validation\n",
    "    test_dataset = dataset[\"test\"].map(preprocess)\n",
    "    \n",
    "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "def train_word2vec(dataset, tokenizer):\n",
    "    sentences = [tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True).split() for example in dataset]\n",
    "    model = Word2Vec(sentences, vector_size=Config.embed_dim, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Student RNN Model\n",
    "class RNNStudent(nn.Module):\n",
    "    def __init__(self, word2vec_model, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(RNNStudent, self).__init__()\n",
    "        embedding_matrix = torch.FloatTensor(word2vec_model.wv.vectors)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=Config.dropout)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(Config.dropout)\n",
    "        self.projection = nn.Linear(hidden_dim, 768)  # New projection layer\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        rnn_output, _ = self.rnn(embedded)\n",
    "        projected_seq = self.projection(rnn_output)\n",
    "        attn_weights = torch.softmax(self.attention(rnn_output).squeeze(-1), dim=-1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), rnn_output).squeeze(1)\n",
    "        output = self.dropout(context)\n",
    "        logits = self.fc(output)\n",
    "        return logits, projected_seq\n",
    "\n",
    "# Compute hybrid loss\n",
    "def compute_kd_loss(student_logits, teacher_logits, student_seq, teacher_seq, labels, attention_mask):\n",
    "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "        nn.functional.log_softmax(student_logits / Config.T, dim=-1),\n",
    "        nn.functional.softmax(teacher_logits / Config.T, dim=-1)\n",
    "    ) * (Config.T ** 2)\n",
    "    valid = attention_mask.unsqueeze(-1).expand_as(student_seq).float()\n",
    "    seq_loss = ((student_seq - teacher_seq) ** 2 * valid).sum() / valid.sum()\n",
    "    hard_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "    return Config.alpha * soft_loss + Config.beta * seq_loss + (1 - Config.alpha - Config.beta) * hard_loss\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate(model, dataloader, device, is_teacher=False):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)  # Fix: Use \"labels\" directly, no stack needed\n",
    "            if is_teacher:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits, _ = model(input_ids, attention_mask)\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    f1 = np.mean([1 if p == l else 0 for p, l in zip(all_preds, all_labels)])  # Simplified F1\n",
    "    return total_loss / len(dataloader), accuracy, f1\n",
    "\n",
    "# Fine-tune teacher (BERT)\n",
    "def fine_tune_teacher(teacher, train_dataloader, val_dataloader, device):\n",
    "    teacher.train()\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": teacher.bert.parameters(), \"lr\": Config.teacher_lr[\"bert\"]},\n",
    "        {\"params\": teacher.classifier.parameters(), \"lr\": Config.teacher_lr[\"classifier\"]}\n",
    "    ])\n",
    "    total_steps = len(train_dataloader) * Config.teacher_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(Config.warmup_steps * total_steps), num_training_steps=total_steps)\n",
    "    \n",
    "    for epoch in range(Config.teacher_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"[Teacher] Epoch {epoch+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)  # Fix: Use \"labels\" directly, no stack needed\n",
    "            teacher.zero_grad()\n",
    "            outputs = teacher(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        val_loss, val_acc, val_f1 = evaluate(teacher, val_dataloader, device, is_teacher=True)\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Train Loss: {avg_train_loss}\")\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Val Loss: {val_loss}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Train student with KD and different learning rates\n",
    "def train_student(teacher, student, train_dataloader, val_dataloader, device):\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": student.embedding.parameters(), \"lr\": Config.student_lr[\"embedding\"]},\n",
    "        {\"params\": student.rnn.parameters(), \"lr\": Config.student_lr[\"rnn\"]},\n",
    "        {\"params\": student.attention.parameters(), \"lr\": Config.student_lr[\"attention\"]},\n",
    "        {\"params\": student.fc.parameters(), \"lr\": Config.student_lr[\"classifier\"]},\n",
    "        {\"params\": student.projection.parameters(), \"lr\": Config.student_lr[\"classifier\"]}\n",
    "    ])\n",
    "    total_steps = len(train_dataloader) * Config.student_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(Config.warmup_steps * total_steps), num_training_steps=total_steps)\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(Config.student_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"[KD] Epoch {epoch+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            student.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(input_ids, attention_mask=attention_mask)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "                teacher_seq = teacher.bert(input_ids, attention_mask=attention_mask)[0]\n",
    "            student_logits, student_seq = student(input_ids, attention_mask)\n",
    "            loss = compute_kd_loss(student_logits, teacher_logits, student_seq, teacher_seq, labels, attention_mask)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        val_loss, val_acc, val_f1 = evaluate(student, val_dataloader, device)\n",
    "        print(f\"[KD] Epoch {epoch+1}, Train Loss: {avg_train_loss}\")\n",
    "        print(f\"[KD] Epoch {epoch+1}, Val Loss: {val_loss}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(student.state_dict(), \"best_student.pt\")\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= Config.patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load data\n",
    "    train_dataset, val_dataset, test_dataset, tokenizer = load_imdb_data()\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, collate_fn=data_collator)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=Config.batch_size, collate_fn=data_collator)\n",
    "    \n",
    "    # Train Word2Vec\n",
    "    word2vec_model = train_word2vec(train_dataset, tokenizer)\n",
    "    \n",
    "    # Initialize models\n",
    "    teacher = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(Config.device)\n",
    "    student = RNNStudent(word2vec_model, tokenizer.vocab_size, Config.embed_dim, Config.hidden_dim).to(Config.device)\n",
    "    \n",
    "    # Apply pruning\n",
    "    apply_pruning(student)\n",
    "    \n",
    "    # Fine-tune teacher\n",
    "    fine_tune_teacher(teacher, train_dataloader, val_dataloader, Config.device)\n",
    "    \n",
    "    # Train student\n",
    "    train_student(teacher, student, train_dataloader, val_dataloader, Config.device)\n",
    "    \n",
    "    # Load best student model\n",
    "    student.load_state_dict(torch.load(\"best_student.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_f1 = evaluate(student, test_dataloader, Config.device)\n",
    "    print(f\"[KD] Final Test Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Print efficiency metrics\n",
    "    print(\"Student Model Efficiency Metrics:\")\n",
    "    print(f\"Parameter Count: {count_parameters(student)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in fine tuning Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"in fine tuning Device: \", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
