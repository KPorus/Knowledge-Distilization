{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, DataCollatorWithPadding\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "from torch.nn.utils import prune\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 32\n",
    "    max_len = 128\n",
    "    teacher_epochs = 3\n",
    "    student_epochs = 10\n",
    "    teacher_lr = {\"bert\": 1e-5, \"classifier\": 5e-5}\n",
    "    student_lr = {\n",
    "        \"embedding\": 1e-4,  # Lower rate for pre-trained embeddings\n",
    "        \"rnn\": 5e-4,       # Higher rate for LSTM\n",
    "        \"attention\": 3e-4, # Moderate rate for attention\n",
    "        \"classifier\": 1e-3 # Higher rate for task-specific layer\n",
    "    }\n",
    "    warmup_steps = 0.1  # 10% of total steps\n",
    "    T = 3.2  # Temperature for KD\n",
    "    alpha = 0.4  # Weight for soft loss\n",
    "    beta = 0.2  # Weight for sequence loss\n",
    "    patience = 5  # Early stopping patience\n",
    "    embed_dim = 100  # Word2Vec embedding dimension\n",
    "    hidden_dim = 256  # RNN hidden dimension\n",
    "    dropout = 0.3  # Dropout rate\n",
    "    prune_amount = 0.3  # Pruning percentage\n",
    "\n",
    "# Load and preprocess IMDB dataset\n",
    "def load_imdb_data():\n",
    "    dataset = load_dataset(\"imdb\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    def preprocess(example):\n",
    "        encoding = tokenizer(\n",
    "            example[\"text\"],\n",
    "            max_length=Config.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": example[\"label\"]\n",
    "        }\n",
    "    \n",
    "    train_dataset = dataset[\"train\"].map(preprocess)\n",
    "    val_dataset = dataset[\"test\"].map(preprocess)  # Using test split as validation\n",
    "    test_dataset = dataset[\"test\"].map(preprocess)\n",
    "    \n",
    "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset, tokenizer\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "def train_word2vec(dataset, tokenizer):\n",
    "    sentences = [tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True).split() for example in dataset]\n",
    "    model = Word2Vec(sentences, vector_size=Config.embed_dim, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Student RNN Model\n",
    "class RNNStudent(nn.Module):\n",
    "    def __init__(self, word2vec_model, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(RNNStudent, self).__init__()\n",
    "        # Embedding layer using Word2Vec weights\n",
    "        embedding_matrix = torch.FloatTensor(word2vec_model.wv.vectors)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embedding.weight = nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = True  # Allow fine-tuning\n",
    "        \n",
    "        # 2-layer LSTM with dropout\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=2, batch_first=True, dropout=Config.dropout)\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(Config.dropout)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        embedded = self.embedding(input_ids)  # [batch, seq_len, embed_dim]\n",
    "        rnn_output, _ = self.rnn(embedded)  # [batch, seq_len, hidden_dim]\n",
    "        \n",
    "        # Attention weights\n",
    "        attn_weights = torch.softmax(self.attention(rnn_output).squeeze(-1), dim=-1)  # [batch, seq_len]\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), rnn_output).squeeze(1)  # [batch, hidden_dim]\n",
    "        \n",
    "        output = self.dropout(context)\n",
    "        logits = self.fc(output)  # [batch, num_classes]\n",
    "        return logits, rnn_output\n",
    "\n",
    "# Apply pruning to student model\n",
    "def apply_pruning(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name=\"weight\", amount=Config.prune_amount)\n",
    "            if module.bias is not None:\n",
    "                prune.l1_unstructured(module, name=\"bias\", amount=Config.prune_amount)\n",
    "        elif isinstance(module, nn.LSTM):\n",
    "            # Prune all weight and bias parameters of LSTM\n",
    "            for param_name in list(module._parameters.keys()):\n",
    "                if \"weight\" in param_name:\n",
    "                    prune.l1_unstructured(module, name=param_name, amount=Config.prune_amount)\n",
    "                elif \"bias\" in param_name and module._parameters[param_name] is not None:\n",
    "                    prune.l1_unstructured(module, name=param_name, amount=Config.prune_amount)\n",
    "\n",
    "# Compute hybrid loss\n",
    "def compute_kd_loss(student_logits, teacher_logits, student_seq, teacher_seq, labels):\n",
    "    # Soft loss (KL divergence)\n",
    "    soft_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n",
    "        nn.functional.log_softmax(student_logits / Config.T, dim=-1),\n",
    "        nn.functional.softmax(teacher_logits / Config.T, dim=-1)\n",
    "    ) * (Config.T ** 2)\n",
    "    \n",
    "    # Sequence-level loss (MSE on hidden states)\n",
    "    seq_loss = nn.MSELoss()(student_seq, teacher_seq)\n",
    "    \n",
    "    # Hard loss (cross-entropy)\n",
    "    hard_loss = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "    \n",
    "    # Hybrid loss\n",
    "    return Config.alpha * soft_loss + Config.beta * seq_loss + (1 - Config.alpha - Config.beta) * hard_loss\n",
    "\n",
    "# Evaluate model\n",
    "def evaluate(model, dataloader, device, is_teacher=False):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = torch.stack(batch[\"labels\"]).to(device)  # Fix: Use \"labels\" and convert to tensor\n",
    "            if is_teacher:\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "            else:\n",
    "                logits, _ = model(input_ids, attention_mask)\n",
    "                loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    f1 = np.mean([1 if p == l else 0 for p, l in zip(all_preds, all_labels)])  # Simplified F1\n",
    "    return total_loss / len(dataloader), accuracy, f1\n",
    "\n",
    "# Fine-tune teacher (BERT)\n",
    "def fine_tune_teacher(teacher, train_dataloader, val_dataloader, device):\n",
    "    teacher.train()\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": teacher.bert.parameters(), \"lr\": Config.teacher_lr[\"bert\"]},\n",
    "        {\"params\": teacher.classifier.parameters(), \"lr\": Config.teacher_lr[\"classifier\"]}\n",
    "    ])\n",
    "    total_steps = len(train_dataloader) * Config.teacher_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(Config.warmup_steps * total_steps), num_training_steps=total_steps)\n",
    "    \n",
    "    for epoch in range(Config.teacher_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_dataloader, desc=f\"[Teacher] Epoch {epoch+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = torch.stack(batch[\"labels\"]).to(device)  # Fix: Use \"labels\" and convert to tensor\n",
    "            teacher.zero_grad()\n",
    "            outputs = teacher(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        val_loss, val_acc, val_f1 = evaluate(teacher, val_dataloader, device, is_teacher=True)\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Train Loss: {avg_train_loss}\")\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Val Loss: {val_loss}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Train student with KD and different learning rates\n",
    "def train_student(teacher, student, train_dataloader, val_dataloader, device):\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    optimizer = optim.AdamW([\n",
    "        {\"params\": student.embedding.parameters(), \"lr\": Config.student_lr[\"embedding\"]},\n",
    "        {\"params\": student.rnn.parameters(), \"lr\": Config.student_lr[\"rnn\"]},\n",
    "        {\"params\": student.attention.parameters(), \"lr\": Config.student_lr[\"attention\"]},\n",
    "        {\"params\": student.fc.parameters(), \"lr\": Config.student_lr[\"classifier\"]}\n",
    "    ])\n",
    "    total_steps = len(train_dataloader) * Config.student_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(Config.warmup_steps * total_steps), num_training_steps=total_steps)\n",
    "    \n",
    "    best_val_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(Config.student_epochs):\n",
    "        total_loss = 0\n",
    "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "            with record_function(\"model_training\"):\n",
    "                for batch in tqdm(train_dataloader, desc=f\"[KD] Epoch {epoch+1}\"):\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    labels = torch.stack(batch[\"labels\"]).to(device)  # Fix: Use \"labels\" and convert to tensor\n",
    "                    student.zero_grad()\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher(input_ids, attention_mask=attention_mask)\n",
    "                        teacher_logits = teacher_outputs.logits\n",
    "                        teacher_seq = teacher.bert(input_ids, attention_mask=attention_mask)[0]  # Last hidden state\n",
    "                    \n",
    "                    student_logits, student_seq = student(input_ids, attention_mask)\n",
    "                    loss = compute_kd_loss(student_logits, teacher_logits, student_seq, teacher_seq, labels)\n",
    "                    total_loss += loss.item()\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        val_loss, val_acc, val_f1 = evaluate(student, val_dataloader, device)\n",
    "        print(f\"[KD] Epoch {epoch+1}, Train Loss: {avg_train_loss}\")\n",
    "        print(f\"[KD] Epoch {epoch+1}, Val Loss: {val_loss}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(student.state_dict(), \"best_student.pt\")\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= Config.patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "# Count model parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load data\n",
    "    train_dataset, val_dataset, test_dataset, tokenizer = load_imdb_data()\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, collate_fn=data_collator)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, collate_fn=data_collator)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=Config.batch_size, collate_fn=data_collator)\n",
    "    \n",
    "    # Train Word2Vec\n",
    "    word2vec_model = train_word2vec(train_dataset, tokenizer)\n",
    "    \n",
    "    # Initialize models\n",
    "    teacher = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(Config.device)\n",
    "    student = RNNStudent(word2vec_model, tokenizer.vocab_size, Config.embed_dim, Config.hidden_dim).to(Config.device)\n",
    "    \n",
    "    # Apply pruning\n",
    "    apply_pruning(student)\n",
    "    \n",
    "    # Fine-tune teacher\n",
    "    fine_tune_teacher(teacher, train_dataloader, val_dataloader, Config.device)\n",
    "    \n",
    "    # Train student\n",
    "    train_student(teacher, student, train_dataloader, val_dataloader, Config.device)\n",
    "    \n",
    "    # Load best student model\n",
    "    student.load_state_dict(torch.load(\"best_student.pt\"))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc, test_f1 = evaluate(student, test_dataloader, Config.device)\n",
    "    print(f\"[KD] Final Test Accuracy: {test_acc:.4f}, F1 Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Print efficiency metrics\n",
    "    print(\"Student Model Efficiency Metrics:\")\n",
    "    print(f\"Parameter Count: {count_parameters(student)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
