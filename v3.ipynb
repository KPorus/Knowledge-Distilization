{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 25000/25000 [01:15<00:00, 331.02 examples/s]\n",
      "Map: 100%|██████████| 25000/25000 [01:04<00:00, 386.01 examples/s]\n",
      "Map: 100%|██████████| 50000/50000 [02:10<00:00, 382.23 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Teacher] Epoch 1, Train Loss: 0.3566975662469864\n",
      "[Teacher] Epoch 1, Val Loss: 0.29004416553078183, Accuracy: 0.8748, F1 Score: 0.8747\n",
      "[Teacher] Epoch 2, Train Loss: 0.23664038577079774\n",
      "[Teacher] Epoch 2, Val Loss: 0.3073555787989668, Accuracy: 0.8740, F1 Score: 0.8738\n",
      "[Teacher] Epoch 3, Train Loss: 0.17687230843007565\n",
      "[Teacher] Epoch 3, Val Loss: 0.32172115856579914, Accuracy: 0.8730, F1 Score: 0.8729\n",
      "[KD] Epoch 1, Train Loss: 0.8141248406410218\n",
      "[KD] Epoch 1, Val Loss: 0.4488598212694666, Accuracy: 0.7910, F1 Score: 0.7909\n",
      "[KD] Epoch 2, Train Loss: 0.48520623807907104\n",
      "[KD] Epoch 2, Val Loss: 0.4178249608179566, Accuracy: 0.8138, F1 Score: 0.8128\n",
      "[KD] Epoch 3, Train Loss: 0.36743490434885023\n",
      "[KD] Epoch 3, Val Loss: 0.372931222864397, Accuracy: 0.8368, F1 Score: 0.8367\n",
      "[KD] Epoch 4, Train Loss: 0.2849605684518814\n",
      "[KD] Epoch 4, Val Loss: 0.37012320614544447, Accuracy: 0.8434, F1 Score: 0.8434\n",
      "[KD] Epoch 5, Train Loss: 0.2238520857155323\n",
      "[KD] Epoch 5, Val Loss: 0.39473469009634793, Accuracy: 0.8456, F1 Score: 0.8455\n",
      "[KD] Epoch 6, Train Loss: 0.17777857791781426\n",
      "[KD] Epoch 6, Val Loss: 0.4083717558177034, Accuracy: 0.8476, F1 Score: 0.8476\n",
      "Student Model Efficiency Metrics:\n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "              aten::embedding        43.87%      37.953ms        79.68%      68.928ms      68.928ms             1  \n",
      "           aten::index_select        34.66%      29.984ms        35.79%      30.956ms      30.956ms             1  \n",
      "                   aten::lstm         2.95%       2.550ms        10.40%       8.999ms       8.999ms             1  \n",
      "       aten::mkldnn_rnn_layer         7.03%       6.080ms         7.05%       6.095ms       6.095ms             1  \n",
      "                 aten::linear         4.73%       4.089ms         5.96%       5.158ms       2.579ms             2  \n",
      "                  aten::zeros         3.66%       3.165ms         3.67%       3.174ms       1.587ms             2  \n",
      "                 aten::matmul         0.02%      17.700us         1.16%       1.000ms       1.000ms             1  \n",
      "                 aten::select         1.13%     973.200us         1.13%     978.700us     244.675us             4  \n",
      "                aten::reshape         0.42%     364.700us         0.61%     525.800us     262.900us             2  \n",
      "                     aten::mm         0.54%     471.200us         0.55%     471.800us     471.800us             1  \n",
      "-----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 86.503ms\n",
      "\n",
      "Student Model Parameter Count: 3170347\n",
      "[KD] Final Test Accuracy: 0.8353, F1 Score: 0.8353\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.profiler\n",
    "\n",
    "# 1) Define RNN student (no direct Word2Vec arg)\n",
    "class RNNStudent(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, _) = self.rnn(embedded)\n",
    "        weights = torch.softmax(self.attention(output).squeeze(-1), dim=1).unsqueeze(-1)\n",
    "        context = torch.sum(output * weights, dim=1)\n",
    "        return self.fc(context)\n",
    "\n",
    "# 2) Distillation loss (unchanged)\n",
    "def distillation_loss(student_logits, teacher_logits, labels, T=3.0, alpha=0.4, beta=0.2):\n",
    "    soft  = nn.KLDivLoss(reduction='batchmean')(\n",
    "                nn.functional.log_softmax(student_logits / T, dim=1),\n",
    "                nn.functional.softmax(teacher_logits / T, dim=1)\n",
    "            ) * (T * T)\n",
    "    hard  = nn.CrossEntropyLoss()(student_logits, labels)\n",
    "    seq_l = nn.CrossEntropyLoss()(student_logits, teacher_logits.argmax(dim=1))\n",
    "    return alpha * soft + (1 - alpha - beta) * hard + beta * seq_l\n",
    "\n",
    "# Fine-Tune Teacher Model with Multiple Learning Rates\n",
    "def fine_tune_teacher(teacher, train_loader, val_loader, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    teacher.train()\n",
    "    teacher.to(device)\n",
    "\n",
    "    # Define parameter groups for multiple learning rates\n",
    "    param_groups = [\n",
    "        {\"params\": [p for n, p in teacher.named_parameters() if \"classifier\" not in n], \"lr\": 1e-5},  # Lower LR for BERT layers\n",
    "        {\"params\": [p for n, p in teacher.named_parameters() if \"classifier\" in n], \"lr\": 5e-5}    # Higher LR for classifier head\n",
    "    ]\n",
    "    optimizer = optim.Adam(param_groups)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Linear scheduler with warmup\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        teacher.train()\n",
    "        for batch in train_loader:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = teacher(input_ids=texts, attention_mask=attention_mask).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        teacher.eval()\n",
    "        preds, true_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                outputs = teacher(input_ids=texts, attention_mask=attention_mask).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(true_labels, preds)\n",
    "        f1 = f1_score(true_labels, preds, average='weighted')\n",
    "        print(f\"[Teacher] Epoch {epoch+1}, Val Loss: {val_loss/len(val_loader)}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Training Function for Knowledge Distillation\n",
    "def train_kd(teacher, student, train_loader, val_loader, epochs=6, prune_amount=0.3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.0005)\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # Apply pruning to student\n",
    "    for name, module in student.named_modules():\n",
    "        if isinstance(module, nn.LSTM):\n",
    "            prune.l1_unstructured(module, name='weight_ih_l0', amount=prune_amount)\n",
    "            prune.l1_unstructured(module, name='weight_hh_l0', amount=prune_amount)\n",
    "        elif isinstance(module, nn.Linear) and module != student.attention:\n",
    "            prune.l1_unstructured(module, name='weight', amount=prune_amount)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        student.train()\n",
    "        for batch in train_loader:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(input_ids=texts, attention_mask=attention_mask).logits\n",
    "            student_outputs = student(texts)\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"[KD] Epoch {epoch+1}, Train Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        preds, true_labels = [], []\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                texts = batch['input_ids'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                outputs = student(texts)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(true_labels, preds)\n",
    "        f1 = f1_score(true_labels, preds, average='weighted')\n",
    "        print(f\"[KD] Epoch {epoch+1}, Val Loss: {val_loss/len(val_loader)}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Efficiency Metrics\n",
    "    student.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['input_ids'].to(device)\n",
    "            with torch.profiler.profile(record_shapes=True) as prof:\n",
    "                student(texts)\n",
    "            break\n",
    "    print(\"Student Model Efficiency Metrics:\")\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "    param_count = sum(p.numel() for p in student.parameters() if p.requires_grad)\n",
    "    print(f\"Student Model Parameter Count: {param_count}\")\n",
    "\n",
    "# Load and Preprocess Data\n",
    "# Load & tokenize IMDB\n",
    "dataset   = load_dataset(\"imdb\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], padding=\"max_length\",\n",
    "        truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "tokenized = (\n",
    "    dataset\n",
    "    .map(tokenize_fn, batched=True)\n",
    "    .rename_column(\"label\", \"labels\")\n",
    "    .remove_columns(\"text\")\n",
    ")\n",
    "tokenized.set_format(\"torch\")\n",
    "split = tokenized[\"train\"].train_test_split(test_size=0.2)\n",
    "train_ds, val_ds = split[\"train\"], split[\"test\"]\n",
    "test_ds = tokenized[\"test\"]\n",
    "\n",
    "collator    = DataCollatorWithPadding(tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  collate_fn=collator)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, collate_fn=collator)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, collate_fn=collator)\n",
    "\n",
    "# 3) Train Word2Vec on raw-words\n",
    "sentences = [text.split() for text in dataset['train']['text']]\n",
    "w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 4) Build hybrid embedding matrix\n",
    "vocab_size = tokenizer.vocab_size   # 30522\n",
    "embed_dim  = 100\n",
    "\n",
    "# start random, then overwrite known tokens\n",
    "emb_matrix = np.random.normal(size=(vocab_size, embed_dim)).astype(np.float32)\n",
    "for token, idx in tokenizer.vocab.items():\n",
    "    if token in w2v.wv:\n",
    "        emb_matrix[idx] = w2v.wv[token]\n",
    "\n",
    "# 5) Initialize models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "student = RNNStudent(vocab_size, embed_dim, hidden_dim=128, output_dim=2).to(device)\n",
    "\n",
    "# copy in the hybrid embeddings\n",
    "student.embedding.weight.data.copy_(torch.from_numpy(emb_matrix))\n",
    "# Fine-Tune Teacher and Perform Knowledge Distillation\n",
    "fine_tune_teacher(teacher, train_loader, val_loader, epochs=3)\n",
    "train_kd(teacher, student, train_loader, val_loader, epochs=6)\n",
    "\n",
    "# Final Test Evaluation\n",
    "student.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "student.to(device)\n",
    "preds, true_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        texts = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = student(texts)\n",
    "        preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "accuracy = accuracy_score(true_labels, preds)\n",
    "f1 = f1_score(true_labels, preds, average='weighted')\n",
    "print(f\"[KD] Final Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
